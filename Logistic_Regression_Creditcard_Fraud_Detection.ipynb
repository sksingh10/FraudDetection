{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Credit Card Fraud Detection \n",
    "Each row in `fraud_data.csv` corresponds to a credit card transaction. Features include confidential variables `V1` through `V28` as well as `Amount` which is the amount of the transaction. \n",
    " \n",
    "The target is stored in the `class` column, where a value of 1 corresponds to an instance of fraud and 0 corresponds to an instance of not fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data \n",
    "Load the data from `fraud_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of observations that are instances of fraud:  0.016410823768035772\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('fraud_data.csv')\n",
    "\n",
    "## Print the percentage of fraud observations\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]\n",
    "\n",
    "# Use X_train, X_test, y_train, y_test for all of the following questions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)  # Your code here\n",
    "\n",
    "print('Percentage of observations that are instances of fraud: ', len(y[y[:]==1])/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What percentage of the observations in the dataset are instances of fraud?\n",
    "\n",
    "- According to the dataset, around 1.6% of the observations are instances if fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions using the majority class label \n",
    "\n",
    "Using `X_train`, `X_test`, `y_train`, and `y_test` (as defined above), train a dummy classifier that classifies everything as the majority class of the training data. What is the accuracy of this classifier? (Here accuracy is the ratio of the number of correctly classified transactions to the total number of transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier accuray: 0.9852507374631269\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "    \n",
    "## Instantiate and fit a dummy classifier that always predict class label by the majority class of the training data\n",
    "## Use DummyClassifier in sklearn with strategy 'most_frequent\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "dummy_test_pred = dummy.predict(X_test)\n",
    "\n",
    "## Measure test accuracy of your dummy classifier\n",
    "dummy_test_acc = accuracy_score(y_test, dummy_test_pred)\n",
    "\n",
    "print('Dummy classifier accuray:', dummy_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** *How does the accuracy of the dummy classifier look (very low, low, high, very high)? Give an explanation.*\n",
    "\n",
    "- The accuracy of the dummy classifier is approximately 98%. This is very high accuracy, as this means that the dummy classifier is able to predict 98% of the values accurately. \n",
    "- However, as we noticed that the number of observations that were instances of fraud were only 1.6%. This means that majority of the observations were not fraud. As the dummy classifier always predicts the class label by the majority class of the training data, it was bound to get most of the values right as most of the values were cases of not fraud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** *How many fraudulent transactions are correctly classified? (This is the **recall** score/measure)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier recall: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "## Measure test recall score of your dummy classifier\n",
    "dummy_test_recall = recall_score(y_test, dummy_test_pred)\n",
    "\n",
    "print('Dummy classifier recall:', dummy_test_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** *How does the recall of the dummy classifier look (very low, low, high, very high)? Give an explanation.*\n",
    "\n",
    "- The recall score of the dummy classifier is 0, which is very low. This happens because the dummy classifier will always predict the class label by the majority class of the training data, which is 'not fraud' in our case. \n",
    "- The dummy classifier, hence, will never assign a 'fraud' label to any of the observations in the test data, which is why the recall score is 0. It will never accurately identify a fraud, as fraud is not a majority class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a logistic regression model \n",
    "\n",
    "Train a logisitic regression classifier with default parameters using X_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic classifier accuray: 0.9964970501474927\n",
      "Logistic classifier recall: 0.7875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "## Instantiate a logistic regression model and fit to the training data\n",
    "logR = LogisticRegression(max_iter=1000)\n",
    "logR.fit(X_train, y_train)\n",
    "\n",
    "logR_test_pred = logR.predict(X_test)\n",
    "\n",
    "## Measure test accuracy \n",
    "logR_test_acc = accuracy_score(y_test, logR_test_pred)\n",
    "\n",
    "print('Logistic classifier accuray:', logR_test_acc)\n",
    "\n",
    "## Measure test recall\n",
    "logR_test_recall = recall_score(y_test, logR_test_pred)\n",
    "\n",
    "print('Logistic classifier recall:', logR_test_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** *Compare the results of logistic regression with those of the above dummy classifier*\n",
    "\n",
    "- Accuracy Score - Comparing the accuracy score of the two models, we can see that the Accuracy score of the logistic Regression model is higher than that of the Dummy Classifier. The accuracy of the logistic Regression model is 99%, which means that the Logistic Regression model got 99% of the predictions right, as compared to the Dummy Classifier which got 98% of the predictions right. \n",
    "\n",
    "- Recall Score - Comparing the recall score of the two models, we can see that the recall score of the Logistic Regression model is much, much higher than that of the Dummy Classifier. The recall score is the number of correct positive predictions made out of all positive predictions. The Logistic Regression model has a recall score of about 78% as compared to that of the Dummy Classifier which has a recall score of 0. This means that the Logistic Regression model was able to accurately identify 78% of the fraud cases as fraud, but the dummy classifier couldn't identify a single fraud case as fraud (because the dummy classifier classified all the cases as 'not fraud' as 'not fraud' was the majority class label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for selecting hyperparameters for Logistic Regression\n",
    "\n",
    "Perform a grid search over the parameters listed below for a Logisitic Regression classifier, using recall for scoring and the default 3-fold cross validation.\n",
    "\n",
    "`'penalty': ['l1', 'l2']`\n",
    "\n",
    "`'C':[0.01, 0.1, 1, 10, 100]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic classifier accuray: 0.9966814159292036\n",
      "Logistic classifier recall: 0.8\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## Define the grid of logistic regression parameters\n",
    "parameters = [{'penalty': ['l1', 'l2'],'C':[0.01, 0.1, 1, 10, 100] }]\n",
    "model = LogisticRegression()\n",
    "    \n",
    "## Perform grid search CV to find best model parameter setting\n",
    "cmodel = GridSearchCV(model, parameters, cv=3, scoring='recall')\n",
    "cmodel.fit(X_train, y_train)\n",
    "\n",
    "## Fit logistic regression with best parameters to the entire training data\n",
    "model = LogisticRegression(C=cmodel.best_params_.get('C'), penalty=cmodel.best_params_.get('penalty') )\n",
    "model.fit(X_train, y_train)\n",
    "    \n",
    "logR_test_pred = model.predict(X_test)\n",
    "\n",
    "## Measure test accuracy\n",
    "logR_test_acc = accuracy_score(y_test, logR_test_pred)\n",
    "\n",
    "print('Logistic classifier accuray:', logR_test_acc)\n",
    "\n",
    "## Measure test recall\n",
    "logR_test_recall = recall_score(y_test, logR_test_pred)\n",
    "\n",
    "print('Logistic classifier recall:', logR_test_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** *Compare the results with that of logistic regression with default parameters*\n",
    "\n",
    "- Accuracy Score - The accuracy score of the Logistic Regression model with default parameters and with parameters from the GridSearchCV are almost the same - 99.6%\n",
    "- Recall Score - The recall score of the Logistic Regression model with default parameters and with parameters from the GridSearchCV differ by approximately 2%. This means that the parameters estimated using the GridSearchCV are better at predicting the fraud cases as fraud, than the model with default parameters. "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "5yX9Z",
   "launcher_item_id": "eqnV3",
   "part_id": "Msnj0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
